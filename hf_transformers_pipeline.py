# -*- coding: utf-8 -*-
"""HF_Transformers Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10N7c9Qofsc_9MT8Nesth5fI841N5R2xY
"""

!pip install transformers

!pip install transformers[sentencepiece]

pip install "transformers[sentencepiece]"

from transformers import pipeline

classifer = pipeline("sentiment-analysis")
classifer("I Love you")

#classifer = pipeline("sentiment-analysis")
classifer("I Hate you")

#classifer = pipeline("sentiment-analysis")
classifer("I am waiting for you from 5 hours time! where are you till now? ")

classifer(["I have been missing my girl friend!.", "I love her so much!"])

from transformers import pipeline

classifer = pipeline("zero-shot-classification")
classifer("This coding is about basic python",candidate_labels = ["Python", "Machine Learning", "Deep Learning"],)

classifer("This coding is about basic python",candidate_labels = ["Python", "C++", "Rust"],)

classifer("This coding is about basic python",candidate_labels = ["Java", "Postgresql", "Python"],)

from transformers import pipeline

generator = pipeline("text-generation")
generator("I love my girl friend more than any other girl friend because")

generator("I want to become cheif minister of andhra pradesh in 2029 to")

from transformers import pipeline

generator = pipeline("text-generation", model = "distilgpt2")
generator("I love my girl friend more than any other girl friend because", max_length = 30, num_return_sequences = 2,)

from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

english_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
chinese_text = "不要插手巫師的事務, 因為他們是微妙的, 很快就會發怒."

tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="en", tgt_lang="zh")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")

# Tokenize the text
encoded_zh = tokenizer(chinese_text, return_tensors="pt")

generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

eng_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
fin_text = "Älä sekaannu velhojen asioihin, sillä ne ovat hienovaraisia ja nopeasti vihaisia."

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang = "fi_FI")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

# Tokenize the text
encoded_en = tokenizer(eng_text,return_tensors="pt")
encoded_fi = tokenizer(fin_text,return_tensors="pt")

generated_tokens = model.generate(**encoded_en,forced_bos_token_id=tokenizer.lang_code_to_id["en_XX"])
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

generated_tokens = model.generate(**encoded_fi,forced_bos_token_id=tokenizer.lang_code_to_id["fi_FI"])
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

"""Fillmask Pipeline()"""

from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("the president of the united states is <mask> .", top_k=2)

unmasker("the president of the united states is <mask> .", top_k=10)

unmasker("This course will teach you all about <mask> models.", top_k=10)

"""bert-base-cased model"""

from transformers import pipeline

unmasker = pipeline("fill-mask", model = "bert-base-cased")
unmasker("Hello I'm a [MASK] model.")

"""Named entity recognition"""

from transformers import pipeline

ner = pipeline("ner", grouped_entities = True)
ner("My name is Dush and I work at Hugging Face in Brooklyn NY.")

pip install flair

from flair.data import Sentence
from flair.models import SequenceTagger

tagger = SequenceTagger.load('ner')
sentence = Sentence("My name is Dush and I work at Hugging Face in Brooklyn NY.")
tagger.predict(sentence)
print(sentence.to_tagged_string())
# print predicted NER spans
print('The following NER tags are found:')
# iterate over entities and print
for entity in sentence.get_spans('pos'):
    print(entity)

"""Question answering"""

from transformers import pipeline

question_answering = pipeline("question-answering")
question_answering(
    question = "Where do I work?",
    context = "My name is Dush and I work at Hugging Face in Brooklyn NY."
)

"""Summarization"""

from transformers import pipeline

summarizer = pipeline("summarization")
summarizer(
    """
    America has changed dramatically during recent years. Not only has the number of
    graduates in traditional engineering disciplines such as mechanical, civil,
    electrical, chemical, and aeronautical engineering declined, but in most of
    the premier American universities engineering curricula now concentrate on
    and encourage largely the study of engineering science. As a result, there
    are declining offerings in engineering subjects dealing with infrastructure,
    the environment, and related issues, and greater concentration on high
    technology subjects, largely supporting increasingly complex scientific
    developments. While the latter is important, it should not be at the expense
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other
    industrial countries in Europe and Asia, continue to encourage and advance
    the teaching of engineering. Both China and India, respectively, graduate
    six and eight times as many traditional engineers as does the United States.
    Other industrial countries at minimum maintain their output, while America
    suffers an increasingly serious decline in the number of engineering graduates
    and a lack of well-educated engineers.
"""
)

"""Translation"""

from transformers import pipeline

translator = pipeline("translation", model = "Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")

from transformers import pipeline

translator = pipeline("translation", model = "facebook/nllb-200-3.3B")
translator("Ce cours est produit par Hugging Face.")